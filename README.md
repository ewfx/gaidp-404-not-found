# Gen AI Data Profiling

## ğŸ“Œ Project Description
This project is a **data profiling and schema generation system** that integrates **PDF and CSV file handling** with a **FastAPI backend** and a **React frontend**. The system enables:
- **File uploads & downloads** (PDF, CSV)
- **Schema extraction using AI**
- **Rule generation for data validation**
- **Metadata storage in MongoDB**

---

## ğŸš€ Functionality
### 1ï¸âƒ£ File Upload and Management
#### ğŸ“„ PDF Upload:
âœ… Users can upload PDF files via the frontend.  
âœ… The backend processes and stores these files for schema generation and downloads.  

#### ğŸ“Š CSV Upload:
âœ… Users can upload CSV files similarly for data profiling.

---
### 2ï¸âƒ£ File Download
#### ğŸ“„ PDF Download:
ğŸ”¹ Endpoint: `GET /pdf/download/{pdf_id}`  
ğŸ”¹ Backend serves the file using `FileResponse` with MIME type `application/pdf`  

#### ğŸ“Š CSV Download:
ğŸ”¹ Endpoint: `GET /csv/download/{csv_id}`  
ğŸ”¹ MIME type: `text/csv`  

---
### 3ï¸âƒ£ Schema Generation
#### ğŸ“‹ Schema Extraction:
âœ”ï¸ AI extracts structured schemas from PDFs, including **column names, page numbers, and metadata**.  
âœ”ï¸ Uses AI agents (CrewAI class) to automate schema extraction.  

#### ğŸ” Column Extraction:
âœ”ï¸ Extracts specific columns from PDFs for **CSV profiling**.  

#### âš¡ Rule Generation:
âœ”ï¸ Generates rules for **data validation & profiling**.

---
### 4ï¸âƒ£ Frontend Features
âœ… **PDF & CSV Management**: View, upload, and download files.  
âœ… **Drag-and-Drop Upload**: Simple and intuitive file upload experience.  
âœ… **Search & Filter**: Easily locate uploaded files.  

---
### 5ï¸âƒ£ Backend Services
- **MongoDB Integration**: Stores metadata of files, schemas, and rules.  
- **AI-Powered Schema & Rule Extraction**: Uses CrewAI to extract structured data from PDFs.  

---

## ğŸ”— API Endpoints
### ğŸ“„ PDF Endpoints
| Method | Endpoint | Description |
|--------|----------------------|------------------|
| POST | `/pdf/upload` | Upload a PDF file |
| GET | `/pdf/download/{pdf_id}` | Download a specific PDF |
| GET | `/pdf/{pdf_id}` | Retrieve metadata of a specific PDF |
| GET | `/pdf` | Retrieve a list of all PDFs |

### ğŸ“Š CSV Endpoints
| Method | Endpoint | Description |
|--------|----------------------|------------------|
| POST | `/csv/upload` | Upload a CSV file |
| GET | `/csv/download/{csv_id}` | Download a specific CSV |
| GET | `/csv/{csv_id}` | Retrieve metadata of a specific CSV |
| GET | `/csv` | Retrieve a list of all CSVs |

### ğŸ“‹ Schema & Rule Endpoints
| Method | Endpoint | Description |
|--------|----------------------|------------------|
| POST | `/schema/create/{pdf_id}` | Generate a schema for a PDF |
| POST | `/rules` | Generate rules based on extracted schemas |

---

## ğŸ›  Implementation
### 1ï¸âƒ£ Backend (FastAPI)
- **File Handling**: Uses `UploadFile` to handle file uploads, and `FileResponse` for downloads.  
- **AI Agents**: `CrewAI` extracts schema and rules from PDFs.  
- **Database**: MongoDB stores file metadata, schemas, and rules.  
- **Services**: `PdfService`, `CsvService`, and `SchemaService` handle business logic.  

### 2ï¸âƒ£ Frontend (React)
- **Components**:  
  - `PdfCard`: Displays PDFs with options for download/schema generation.  
  - `ExistingPDFList`: Displays a list of uploaded PDFs.  
  - `UploadPDF`: Main interface for managing PDF uploads.  
- **API Integration**: Uses `axios` for API calls.  
- **Routing**: Handled via `React Router`.  
- **Styling**: Uses **CSS modules**.  

### 3ï¸âƒ£ Middleware
- **CORS Middleware**: Configured to allow frontend requests (`http://localhost:5174`).

---

## ğŸŒ Deployment
### â–¶ï¸ Backend:
```sh
uvicorn app.main:app --reload
```

### â–¶ï¸ Frontend:
```sh
cd frontend
npm install
npm run dev
```

---

## âš™ï¸ Key Technologies
### ğŸ”¹ Backend:
- **FastAPI** â€“ REST API framework
- **MongoDB** â€“ NoSQL database
- **CrewAI** â€“ AI-powered schema & rule extraction
- **Python Libraries** â€“ `pdfminer`, `pydantic`

### ğŸ”¹ Frontend:
- **React** â€“ UI Framework
- **Axios** â€“ API requests
- **Vite** â€“ Build tool

### ğŸ”¹ Other Tools:
- **dotenv** â€“ Environment variable management

---

## ğŸ¯ Summary
This project is a **comprehensive data profiling tool** combining:
âœ… AI-powered schema extraction  
âœ… PDF & CSV management  
âœ… Automated rule generation  
âœ… A seamless **FastAPI backend & React frontend** experience  

ğŸš€ **Empower your data profiling with AI today!**


---

## ğŸ–¥ï¸ Steps to Run the Project Locally

### **1ï¸âƒ£ Prerequisites**
Ensure you have the following installed:
- **Python** (version 3.8 or higher)
- **Node.js** (for the frontend)
- **npm** or **yarn** (comes with Node.js)
- **MongoDB** (for metadata storage)

---

### **2ï¸âƒ£ Backend Setup**

#### **Step 1: Create a Python Virtual Environment**
1. Open a terminal and navigate to the `backend` directory:
   ```bash
   cd backend
   ```
2. Create a virtual environment:
   ```bash
   python -m venv myenv
   ```
3. Activate the virtual environment:
   ```bash
   myenv\Scripts\activate  # For Windows
   source myenv/bin/activate  # For macOS/Linux
   ```

#### **Step 2: Install Dependencies**
1. Install the required Python packages:
   ```bash
   pip install -r requirements.txt
   ```

#### **Step 3: Configure Environment Variables**
1. Create a `.env` file in the `backend` directory and add the following:
   ```env
   SAMBANOVA_API_KEY=your_api_key_here
   ```

#### **Step 4: Run the Backend**
1. Start the FastAPI backend:
   ```bash
   uvicorn app.main:app --reload
   ```
2. The backend will be available at: [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

### **3ï¸âƒ£ Frontend Setup**

#### **Step 1: Navigate to the Frontend Directory**
1. Open a new terminal and navigate to the `frontend/my-react-app` directory:
   ```bash
   cd frontend/my-react-app
   ```

#### **Step 2: Install Dependencies**
1. Install the required Node.js packages:
   ```bash
   npm install
   ```

#### **Step 3: Run the Frontend**
1. Start the React development server:
   ```bash
   npm run dev
   ```
2. The frontend will be available at: [http://127.0.0.1:5173](http://127.0.0.1:5173)

---

### **4ï¸âƒ£ MongoDB Setup**
1. Ensure MongoDB is installed and running on your system.
2. By default, the backend will connect to `mongodb://localhost:27017`. If you need to customize the connection, update the MongoDB URI in the backend configuration.

---

### **5ï¸âƒ£ Testing the Application**
1. **Upload Files**:
   - Use the frontend to upload PDF or CSV files.
   - Alternatively, use tools like Postman to test the backend endpoints.
2. **Download Files**:
   - Test the file download functionality for both PDF and CSV files.
3. **Schema Generation**:
   - Verify that the AI-based schema generation works as expected.

---

### **6ï¸âƒ£ Additional Notes**
- **Frontend Build**:
  If you want to build the frontend for production, run:
  ```bash
  npm run build
  ```
  The build files will be available in the `dist` directory.
- **Backend Logs**:
  Check the terminal running the backend for logs and error messages.

---

### **7ï¸âƒ£ Cleanup**
To deactivate the Python virtual environment, run:
```bash
deactivate
```

By following these steps, you should be able to run the **Gen AI Data Profiling** project successfully in your local environment.
